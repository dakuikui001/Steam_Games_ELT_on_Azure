{
	"name": "great_expectations_processing",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "newpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "c0848175-6816-40ac-8ac6-8eb00b22074e"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/39078ff2-6324-4d09-88e5-575820da458d/resourceGroups/data_engineering/providers/Microsoft.Synapse/workspaces/steam-game-synapse/bigDataPools/newpool",
				"name": "newpool",
				"type": "Spark",
				"endpoint": "https://steam-game-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/newpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.5",
				"nodeCount": 10,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import great_expectations as gx\n",
					"from pyspark.sql import functions as F\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
					"import traceback\n",
					"import os\n",
					"import json\n",
					"import threading\n",
					"import gc\n",
					"\n",
					"# ==========================================\n",
					"# 1. Âü∫Á°ÄÈÖçÁΩÆ\n",
					"# ==========================================\n",
					"PLATFORM = \"SYNAPSE\" \n",
					"\n",
					"BASE_PATH = \"abfss://steam-game-project@dataprojectsforhuilu.dfs.core.windows.net/gx_config/expectations/\"\n",
					"QUARANTINE_TABLE = \"data_quality_quarantine\"\n",
					"\n",
					"\n",
					"_SHARED_GX_CONTEXT = None\n",
					"_CACHED_SUITES_JSON = {}\n",
					"gx_lock = threading.RLock() \n",
					"\n",
					"# ==========================================\n",
					"# 2. ÈÖçÁΩÆÈ¢ÑÂä†ËΩΩ (ÈíàÂØπ Synapse ‰ºòÂåñÁâà)\n",
					"# ==========================================\n",
					"def preload_all_suites():\n",
					"    global _CACHED_SUITES_JSON\n",
					"    try:\n",
					"        # ‰ΩøÁî® ls ÂàóÂá∫Êñá‰ª∂\n",
					"        files = mssparkutils.fs.ls(BASE_PATH)\n",
					"        \n",
					"        for f in files:\n",
					"            if f.name.endswith(\".json\"):\n",
					"                suite_name = f.name.replace(\".json\", \"\")\n",
					"                try:\n",
					"                    # ‰ΩøÁî® spark ËØªÂèñÊñá‰ª∂ÂÜÖÂÆπÂπ∂ËΩ¨‰∏∫Â≠óÁ¨¶‰∏≤\n",
					"                    # .collect()[0][0] Ëé∑ÂèñÁ¨¨‰∏ÄË°åÁ¨¨‰∏ÄÂàóÁöÑÂÜÖÂÆπ\n",
					"                    content = spark.read.text(f.path).collect()\n",
					"                    json_str = \"\".join([row[0] for row in content])\n",
					"                    \n",
					"                    suite_dict = json.loads(json_str)\n",
					"                    suite_dict.pop(\"name\", None)\n",
					"                    suite_dict.pop(\"data_context_id\", None)\n",
					"                    \n",
					"                    _CACHED_SUITES_JSON[suite_name] = suite_dict\n",
					"                    print(f\"‚úÖ Preloaded Suite: {suite_name}\")\n",
					"                except Exception as e:\n",
					"                    print(f\"‚ùå Load error {f.name}: {e}\")\n",
					"    except Exception as e:\n",
					"        print(f\"‚ùå Error accessing {BASE_PATH}: {e}\")\n",
					"\n",
					"# ÊâßË°åÈ¢ÑÂä†ËΩΩ\n",
					"preload_all_suites()\n",
					"\n",
					"def get_gx_context():\n",
					"    global _SHARED_GX_CONTEXT\n",
					"    with gx_lock:\n",
					"        if _SHARED_GX_CONTEXT is None:\n",
					"            _SHARED_GX_CONTEXT = gx.get_context(mode=\"ephemeral\")\n",
					"        return _SHARED_GX_CONTEXT\n",
					"\n",
					"def load_suite_simple(context, suite_name):\n",
					"    try:\n",
					"        return context.suites.get(name=suite_name)\n",
					"    except Exception:\n",
					"        if suite_name in _CACHED_SUITES_JSON:\n",
					"            suite_data = _CACHED_SUITES_JSON[suite_name]\n",
					"            new_suite = gx.ExpectationSuite(\n",
					"                name=suite_name, \n",
					"                expectations=suite_data.get(\"expectations\", [])\n",
					"            )\n",
					"            return context.suites.add(new_suite)\n",
					"        else:\n",
					"            raise FileNotFoundError(f\"Suite {suite_name} not found in cache.\")\n",
					"\n",
					"# ==========================================\n",
					"# 3. Ê†∏ÂøÉÂ§ÑÁêÜÂáΩÊï∞\n",
					"# ==========================================\n",
					"def validate_and_insert_process_batch(df, catalog, schema, batch_id, table_name): \n",
					"    spark_internal = df.sparkSession\n",
					"    if df.limit(1).count() == 0: return\n",
					"\n",
					"    full_target_table = f\"{catalog}.{schema}.{table_name}\" if PLATFORM == \"DATABRICKS\" else table_name\n",
					"    temp_id_col = \"_dq_batch_id\"\n",
					"    ds_name = f\"ds_{table_name}_{batch_id}\"\n",
					"    val_def_name = f\"val_{table_name}_{batch_id}\"\n",
					"    \n",
					"    # ÊåÅ‰πÖÂåñ‰ª•Á°Æ‰øù ID Á®≥ÂÆö\n",
					"    df_with_id = df.withColumn(temp_id_col, F.monotonically_increasing_id()).persist()\n",
					"    result = None \n",
					"\n",
					"    # --- ÈîÅÂÜÖÈ™åËØÅÊµÅ ---\n",
					"    with gx_lock:\n",
					"        try:\n",
					"            print(f\"üîí Batch {batch_id}: Processing {table_name}...\", flush=True)\n",
					"            context = get_gx_context()\n",
					"            \n",
					"            # Ê∏ÖÁêÜÊóßÂÆö‰πâ\n",
					"            for n in [val_def_name, ds_name]:\n",
					"                try: \n",
					"                    context.validation_definitions.delete(n) if \"val\" in n else context.data_sources.delete(n)\n",
					"                except: pass\n",
					"\n",
					"            datasource = context.data_sources.add_spark(name=ds_name)\n",
					"            asset = datasource.add_dataframe_asset(name=f\"asset_{batch_id}\")\n",
					"            batch_def = asset.add_batch_definition_whole_dataframe(name=\"batch_def\")\n",
					"            suite = load_suite_simple(context, f\"{table_name}_suite\")\n",
					"            \n",
					"            val_definition = context.validation_definitions.add(\n",
					"                gx.ValidationDefinition(name=val_def_name, data=batch_def, suite=suite)\n",
					"            )\n",
					"\n",
					"            print(f\"üöÄ Batch {batch_id}: Running GX for {table_name}...\", flush=True)\n",
					"            result = val_definition.run(\n",
					"                batch_parameters={\"dataframe\": df_with_id},\n",
					"                result_format={\"result_format\": \"COMPLETE\", \"unexpected_index_column_names\": [temp_id_col]}\n",
					"            )\n",
					"            \n",
					"            if result.success:\n",
					"                print(f\"‚úÖ Batch {batch_id}: {table_name} Passed.\", flush=True)\n",
					"            else:\n",
					"                print(f\"‚ö†Ô∏è Batch {batch_id}: {table_name} FAILED.\", flush=True)\n",
					"\n",
					"            # ÂèäÊó∂ÈáäÊîæ Context ËµÑÊ∫ê\n",
					"            context.validation_definitions.delete(val_def_name)\n",
					"            context.data_sources.delete(ds_name)\n",
					"        except Exception as e:\n",
					"            print(f\"‚ùå Batch {batch_id} GX Error on {table_name}: {str(e)}\", flush=True)\n",
					"            df_with_id.drop(temp_id_col).write.mode(\"append\").saveAsTable(full_target_table)\n",
					"            return \n",
					"        finally:\n",
					"            print(f\"üîì Batch {batch_id}: {table_name} Released Lock.\", flush=True)\n",
					"            gc.collect()\n",
					"\n",
					"    # --- ÈîÅÂ§ñÂÖ•Â∫ìÈÄªËæë  ---\n",
					"    try:\n",
					"        if result and result.success:\n",
					"            df_with_id.drop(temp_id_col).write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(full_target_table)\n",
					"        \n",
					"        elif result:\n",
					"            errors = []\n",
					"            for r in result.results:\n",
					"                if not r.success:\n",
					"                    # ÊèêÂèñË°åÁ∫ßÈîôËØØ ID\n",
					"                    conf = r.expectation_config\n",
					"                    col = conf.kwargs.get(\"column\", \"Table\")\n",
					"                    rule = conf.type\n",
					"                    ids = r.result.get(\"unexpected_index_list\")\n",
					"                    if ids:\n",
					"                        for row_id_dict in ids:\n",
					"                            val = row_id_dict.get(temp_id_col)\n",
					"                            if val is not None:\n",
					"                                errors.append((val, f\"[{col}] {rule}\"))\n",
					"            \n",
					"            # Â¶ÇÊûúÈ™åËØÅÂ§±Ë¥•‰ΩÜÊ≤°Êúâ‰ªª‰ΩïÂÖ∑‰ΩìÁöÑÂùèË°å ID -> Âà§ÂÆö‰∏∫Ë°®Á∫ßÁªìÊûÑÈîôËØØÔºåÊï¥ÊâπÊã¶Êà™\n",
					"            if not errors: \n",
					"                print(f\"üö® Batch {batch_id}: {table_name} TABLE-LEVEL ERROR! Quarantining entire batch.\", flush=True)\n",
					"                # ÊâìÂç∞ÂÖ∑‰ΩìÈîôÂú®Âì™,Â∏ÆÂä©ÊéíÊü•\n",
					"                for r in result.results:\n",
					"                    if not r.success:\n",
					"                        print(f\"   Mismatch Details: {r.result.get('details')}\")\n",
					"                \n",
					"                # ÈöîÁ¶ªÊï¥ÊâπÊï∞ÊçÆ\n",
					"                bad_df = df_with_id.withColumn(\"violated_rules\", F.lit(\"Table-level Schema/Count Error\")) \\\n",
					"                    .withColumn(\"raw_data\", F.to_json(F.struct([c for c in df.columns]))) \\\n",
					"                    .withColumn(\"origin_table\", F.lit(table_name)) \\\n",
					"                    .withColumn(\"ingestion_time\", F.current_timestamp()) \\\n",
					"                    .select(F.col(\"origin_table\").alias(\"table_name\"), F.lit(str(batch_id)).alias(\"gx_batch_id\"),\n",
					"                            \"violated_rules\", \"raw_data\", \"ingestion_time\")\n",
					"                bad_df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(QUARANTINE_TABLE)\n",
					"                return\n",
					"\n",
					"            # Ë°åÁ∫ßÈöîÁ¶ªÈÄªËæë\n",
					"            error_schema = StructType([StructField(temp_id_col, LongType(), True), StructField(\"violated_rule\", StringType(), True)])\n",
					"            error_info_df = spark_internal.createDataFrame(errors, schema=error_schema) \\\n",
					"                .groupBy(temp_id_col).agg(F.concat_ws(\"; \", F.collect_list(\"violated_rule\")).alias(\"violated_rules\"))\n",
					"\n",
					"            bad_row_ids = [e[0] for e in errors]\n",
					"            bad_df = df_with_id.filter(F.col(temp_id_col).isin(bad_row_ids)).join(error_info_df, on=temp_id_col, how=\"left\") \\\n",
					"                .withColumn(\"raw_data\", F.to_json(F.struct([c for c in df.columns]))) \\\n",
					"                .withColumn(\"origin_table\", F.lit(table_name)).withColumn(\"ingestion_time\", F.current_timestamp()) \\\n",
					"                .select(F.col(\"origin_table\").alias(\"table_name\"), F.lit(str(batch_id)).alias(\"gx_batch_id\"),\n",
					"                        \"violated_rules\", \"raw_data\", \"ingestion_time\")\n",
					"            bad_df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(QUARANTINE_TABLE)\n",
					"            \n",
					"            # Â∞ÜÂâ©‰∏ãÁöÑÂ•ΩË°åÂÜôÂÖ•ÁõÆÊ†á\n",
					"            good_df = df_with_id.filter(~F.col(temp_id_col).isin(bad_row_ids)).drop(temp_id_col)\n",
					"            if good_df.limit(1).count() > 0:\n",
					"                good_df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(full_target_table)\n",
					"            print(f\"üì¶ Batch {batch_id}: {table_name} Quarantined {len(set(bad_row_ids))} rows.\", flush=True)\n",
					"\n",
					"    except Exception as e:\n",
					"        print(f\"‚ùå Batch {batch_id} Write Error on {table_name}: {str(e)}\", flush=True)\n",
					"        df_with_id.drop(temp_id_col).write.mode(\"append\").saveAsTable(full_target_table)\n",
					"    finally:\n",
					"        if df_with_id.is_cached: df_with_id.unpersist()\n",
					"        gc.collect()"
				],
				"execution_count": null
			}
		]
	}
}